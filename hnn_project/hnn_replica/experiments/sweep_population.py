"""Sweep HNN population (N) and memory count (P) configurations at zero noise.

This helper runs `MainHNN_SHB.py` for every admissible pair of
(HNN_N, NUM_MEMORIES) within user-provided bounds, collects the
`metrics_summary.csv` artifacts generated by `HNPlot.py`, and evaluates
whether each targeted memory remains above the retention threshold for
an entire stimulus-free interval.
"""

from __future__ import annotations

import argparse
import csv
import math
import os
import subprocess
import sys
from pathlib import Path
from typing import Dict, Iterable, List, Sequence

REPLICA_DIR = Path(__file__).resolve().parent
OUTPUTS_DIR = REPLICA_DIR / "outputs"
PYTHON = Path(sys.executable)


def _snapshot_outputs() -> List[Path]:
    if not OUTPUTS_DIR.exists():
        return []
    return [p for p in OUTPUTS_DIR.iterdir() if p.is_dir()]


def _latest_output(before: Sequence[Path]) -> Path:
    """Return the newest output directory created after a run."""

    before_names = {p.name for p in before}
    if not OUTPUTS_DIR.exists():
        raise RuntimeError("outputs directory missing after simulation run")
    after = [p for p in OUTPUTS_DIR.iterdir() if p.is_dir()]
    if not after:
        raise RuntimeError("no output directories found after simulation run")
    new_dirs = [p for p in after if p.name not in before_names]
    candidates = new_dirs or after
    return max(candidates, key=lambda path: path.stat().st_mtime)


def _read_metrics(directory: Path) -> List[Dict]:
    path = directory / "metrics_summary.csv"
    if not path.exists():
        return []
    with path.open(newline="", encoding="utf-8") as handle:
        reader = csv.DictReader(handle)
        rows = []
        for row in reader:
            rows.append(
                {
                    "segment": int(row["segment"]),
                    "memory": int(row["memory"]),
                    "reaction_mean": float(row["reaction_mean"]) if row["reaction_mean"] else float("nan"),
                    "retention_mean": float(row["retention_mean"]) if row["retention_mean"] else float("nan"),
                }
            )
    return rows


def _read_metadata(directory: Path) -> Dict:
    path = directory / "run_metadata.json"
    if not path.exists():
        return {}
    import json  # Local import to avoid paying the cost when not needed.

    with path.open(encoding="utf-8") as handle:
        return json.load(handle)


def _normalize_n_values(raw: str, n_max: int) -> List[int]:
    values = []
    for token in raw.split(","):
        token = token.strip()
        if not token:
            continue
        val = int(token)
        if 2 <= val <= n_max:
            values.append(val)
    unique_values = sorted(set(values))
    if not unique_values:
        raise ValueError("no valid N values provided")
    return unique_values


def _summarize_retention(metrics: List[Dict], metadata: Dict) -> Dict:
    tail_time = max(metadata.get("segment_duration", 0.0) - metadata.get("stim_duration", 0.0), 0.0)
    dt = metadata.get("dt", 0.01)
    tolerance = max(dt * 5, 0.05)
    details = []
    retained = 0
    for metric in metrics:
        retention = metric["retention_mean"]
        if math.isnan(retention):
            retained_flag = False
        elif tail_time <= 0:
            retained_flag = True
        else:
            retained_flag = retention + tolerance >= tail_time
        retained += int(retained_flag)
        details.append({**metric, "retained_full": retained_flag})
    total = len(details)
    retention_values = [m["retention_mean"] for m in details if not math.isnan(m["retention_mean"])]
    return {
        "tail_time": tail_time,
        "tolerance": tolerance,
        "retained_fraction": (retained / total) if total else 0.0,
        "all_retained": (retained == total) and total > 0,
        "min_retention": min(retention_values) if retention_values else float("nan"),
        "max_retention": max(retention_values) if retention_values else float("nan"),
        "details": details,
    }


def run_config(
    neuron_count: int,
    memory_count: int,
    noise: float,
    trials: int,
    w_s: float,
    w_external: float,
    stim_duration: float,
    t_end: float,
    save_plots: bool,
) -> Dict:
    env = os.environ.copy()
    env.update(
        {
            "HNN_N": str(neuron_count),
            "NUM_MEMORIES": str(memory_count),
            "NOISE_LEVEL": str(noise),
            "W_S": str(w_s),
            "W_EXTERNAL": str(w_external),
            "TRIAL_SAMPLES": str(trials),
            "INPUT_MODE": "deterministic",
            "DETERMINISTIC_GAIN": "3.0",
            "INPUT_LOG_TRIALS": "0",
            "INPUT_PLOT_TRIALS": "0",
            "INPUT_PLOT_SEGMENTS": "0",
            "SAVE_PLOTS": "1" if save_plots else "0",
            "RANDOM_SEED": str(10_000 + neuron_count * 100 + memory_count),
            "STIM_DURATION": str(stim_duration),
            "T_END": str(t_end),
        }
    )
    before = _snapshot_outputs()
    subprocess.run(
        [str(PYTHON), "MainHNN_SHB.py"],
        cwd=str(REPLICA_DIR),
        env=env,
        check=True,
    )
    latest_dir = _latest_output(before)
    metrics = _read_metrics(latest_dir)
    metadata = _read_metadata(latest_dir)
    metadata.setdefault("segment_duration", t_end)
    metadata.setdefault("stim_duration", stim_duration)
    metadata.setdefault("dt", 0.01)
    metadata["output_dir"] = str(latest_dir.relative_to(REPLICA_DIR))
    return {"metadata": metadata, "metrics": metrics}


def _write_csv(path: Path, rows: Iterable[Dict], fieldnames: Sequence[str]) -> None:
    with path.open("w", newline="", encoding="utf-8") as handle:
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        writer.writeheader()
        for row in rows:
            writer.writerow(row)


def _build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Sweep N/P configs at zero noise")
    parser.add_argument("--n-values", default="16,24,32,40,48,56,64", help="Comma-separated list of neuron counts")
    parser.add_argument("--n-max", type=int, default=64, help="Upper bound for neuron counts")
    parser.add_argument("--p-max", type=int, default=10, help="Maximum number of memories per configuration")
    parser.add_argument("--noise", type=float, default=0.0, help="Noise level injected into MainHNN_SHB")
    parser.add_argument("--trials", type=int, default=12, help="Trials (TRIAL_SAMPLES) per configuration")
    parser.add_argument("--w-s", type=float, dest="w_s", default=1.0, help="Short-term weight scalar")
    parser.add_argument("--w-ext", type=float, dest="w_external", default=1.5, help="External drive scalar")
    parser.add_argument("--stim-duration", type=float, default=2.0, help="Stimulus duration in seconds")
    parser.add_argument("--t-end", type=float, default=10.0, help="Segment duration in seconds")
    parser.add_argument("--save-plots", action="store_true", help="Keep overlap PDFs instead of disabling plots")
    return parser


def main() -> None:
    parser = _build_arg_parser()
    args = parser.parse_args()
    try:
        n_values = _normalize_n_values(args.n_values, args.n_max)
    except ValueError as exc:
        parser.error(str(exc))
    aggregate_rows = []
    detail_rows = []
    total_runs = sum(1 for n in n_values for _ in range(1, min(args.p_max, n - 1) + 1))
    run_index = 0
    for n in n_values:
        admissible_p = range(1, min(args.p_max, n - 1) + 1)
        for p in admissible_p:
            run_index += 1
            print(f"[{run_index}/{total_runs}] N={n:2d}, P={p:2d} -> running simulation...")
            try:
                result = run_config(
                    neuron_count=n,
                    memory_count=p,
                    noise=args.noise,
                    trials=args.trials,
                    w_s=args.w_s,
                    w_external=args.w_external,
                    stim_duration=args.stim_duration,
                    t_end=args.t_end,
                    save_plots=args.save_plots,
                )
            except subprocess.CalledProcessError as exc:
                aggregate_rows.append(
                    {
                        "N": n,
                        "P": p,
                        "retained_fraction": 0.0,
                        "all_retained": False,
                        "tail_time": 0.0,
                        "tolerance": 0.0,
                        "min_retention": float("nan"),
                        "max_retention": float("nan"),
                        "output_dir": "",
                        "error": exc.stderr.strip() if exc.stderr else str(exc),
                    }
                )
                print(f"    FAILED -> {exc}")
                continue
            metadata = result["metadata"]
            metrics = result["metrics"]
            retention_summary = _summarize_retention(metrics, metadata)
            aggregate_rows.append(
                {
                    "N": n,
                    "P": p,
                    "retained_fraction": retention_summary["retained_fraction"],
                    "all_retained": retention_summary["all_retained"],
                    "tail_time": retention_summary["tail_time"],
                    "tolerance": retention_summary["tolerance"],
                    "min_retention": retention_summary["min_retention"],
                    "max_retention": retention_summary["max_retention"],
                    "output_dir": metadata["output_dir"],
                    "error": "",
                }
            )
            for detail in retention_summary["details"]:
                detail_rows.append(
                    {
                        "N": n,
                        "P": p,
                        "segment": detail.get("segment"),
                        "memory": detail.get("memory"),
                        "reaction_mean": detail.get("reaction_mean"),
                        "retention_mean": detail.get("retention_mean"),
                        "retained_full": detail.get("retained_full"),
                        "tail_time": retention_summary["tail_time"],
                        "output_dir": metadata["output_dir"],
                    }
                )
            retained_pct = retention_summary["retained_fraction"] * 100
            status = "stable" if retention_summary["all_retained"] else "partial"
            print(f"    -> {status.upper():7s} | retained={retained_pct:5.1f}% | outputs={metadata['output_dir']}")
    summary_path = REPLICA_DIR / "pn_retention_summary.csv"
    detail_path = REPLICA_DIR / "pn_retention_details.csv"
    _write_csv(
        summary_path,
        aggregate_rows,
        ["N", "P", "retained_fraction", "all_retained", "tail_time", "tolerance", "min_retention", "max_retention", "output_dir", "error"],
    )
    _write_csv(
        detail_path,
        detail_rows,
        ["N", "P", "segment", "memory", "reaction_mean", "retention_mean", "retained_full", "tail_time", "output_dir"],
    )
    print(f"\nSaved aggregate summary to {summary_path}")
    print(f"Detailed per-segment metrics stored in {detail_path}")


if __name__ == "__main__":
    main()
